#!/usr/bin/env python 

"""Manipulation of regular and context free languages."""

from __future__ import division

from optparse import OptionParser
from pprint import pprint
import string
import sys
import random

import nltk
from nltk.grammar import parse_cfg, Nonterminal, ContextFreeGrammar, Production

from cnf import convert_to_cnf

class CFLGenerator(object):
    """Random string generation from the language generated by input grammar.
    """

    def __init__(self, grammar, length=1):
        """Convert the grammar to Chomsky Normal Form and do preprocessing.
        
        `grammar` can be:
            (1) an instance of nltk.grammar.ContextFreeGrammar,
            (2) a string representing the path to a .cfg file, or
            (3) a string that can be parsed into a grammar by parse_cfg

        `length` is the maximum string length that should be preprocessed.
        """
        if length < 1:
            raise ValueError('length must be greater than 0.')

        # self.grammar must be instance of nltk.grammar.Grammar
        if isinstance(grammar, ContextFreeGrammar): #TODO: is CFG base class?
            self.grammar = grammar
        elif isinstance(grammar, str) and grammar.endswith('.cfg'):
            self.grammar = nltk.data.load('file:' + grammar)
        elif isinstance(grammar, str):
            self.grammar = parse_cfg(grammar)
        else:
            raise ValueError('Arg grammar must be nltk.grammar.Grammar or str.')
        
        if not self.grammar.is_chomsky_normal_form():
            self.grammar = convert_to_cnf(self.grammar)

        self.productions = self.grammar.productions()

        # NOTE: Is it ok to assume all nonterminals occur on a LHS?
        self.nonterminals = set([p.lhs() for p in self.productions])

        self.terminals = set()
        for prod in self.productions:
            for token in prod.rhs():
                if not isinstance(token, Nonterminal):
                    self.terminals.add(token)

        # Initialize _counts then populate it in _preprocess().
        self._counts = {}
        self._preprocess(length)

    def count_by_nonterm(self, nonterm, length):
        """Return number of strings of length `length` derivable from `nonterm`.
        """
        if isinstance(nonterm, basestring):
            nts = [nt for nt in self.nonterminals if nt.symbol() == nonterm]
            if not nts:
                raise KeyError("nonterm isn't in grammar.")
                #TODO: subclass Exception and print the missing nonterm
            (nonterm,) = nts
        return self._counts[nonterm][length]

    def count_by_prod(self, prod, length):
        """Return the number of strings of length `length` derivable from the
        RHS of `prod`."""
        assert isinstance(prod, Production)
        assert len(prod.rhs()) < 3

        if isinstance(prod.rhs()[0], basestring):
            if length == 1:
                return 1
            else:
                return 0

        B, C = prod.rhs()[0], prod.rhs()[1]
        return sum([self.count_by_nonterm(B, k) *  
                    self.count_by_nonterm(C, length - k)
                    for k in range(1, length)])


    def generate(self, length):
        """Return a string of length `length` that is in L(self.grammar).
        
        Raise GenerationFailure if no strings of the requested length can be 
        generated.
        """
        # Update self._counts for string lengths up to `length` if necessary.
        if length > self.length:
            self._update_counts(length)

        # TODO: GENERATION
        start = self.grammar.start()
        return self._generate_rec(start, length)


    def _generate_rec(self, nonterm, length):
        if length == 1:
            productions = self._prods_by_lhs(nonterm)
            terminals = [p.rhs()[0] for p in productions if len(p.rhs()) == 1]
            if productions and not terminals:
                #TODO: Add exception for when the grammar can't generate a 
                #      string of the requested length.
                raise GenerationFailure(length)
            choice = random.choice(terminals)
            return choice

        #TODO: use listcomps
        for prod in self._prods_by_lhs(nonterm):
            count = self.count_by_prod(prod, length)
        productions = [prod for prod in self._prods_by_lhs(nonterm)
                       if self.count_by_prod(prod, length) > 0]
        #TODO: do probabilities in above listcomp
        probabilities = []
        for prod in productions:
            numerator = self.count_by_prod(prod, length)
            denominator = self.count_by_nonterm(prod.lhs(), length)
            if denominator != 0: 
                probabilities.append(numerator / denominator)
        if not productions:
            raise GenerationFailure(length)
        production = _choose(zip(productions, probabilities))
        if production is None:
            #TODO: Add exception for when the grammar can't generate a string
            #      of the requested length.
            return None
        
        first, second = production.rhs()[0], production.rhs()[1]
        # Calculate split #TODO: listcomp
        split_probs = []
        for k in range(1, length):
            numerator1 = self.count_by_nonterm(first, k)
            numerator2 = self.count_by_nonterm(second, length - k)
            denominator = self.count_by_prod(production, length)
            prob = (numerator1 * numerator2) / denominator
            split_probs.append((k, prob))
        assert split_probs
        split = _choose(split_probs)
        left = self._generate_rec(production.rhs()[0], split)
        right = self._generate_rec(production.rhs()[1], length - split)
        assert isinstance(left, basestring)
        assert isinstance(right, basestring)
        return left + right



    def _prods_by_lhs(self, lhs):
        assert isinstance(lhs, Nonterminal)
        return set([p for p in self.productions if p.lhs() == lhs])


    def _update_counts(self, new_length):
        """Extend self._counts to cover strings of length `new_length`."""
        assert self.length <= max([new_length, 1])

        # Extend the count lists with 0s
        diff = new_length - self.length
        for key, value in self._counts.iteritems():
            value += [0 for i in range(diff)]

        for L in range(self.length + 1, new_length + 1):
            for nonterm in self.nonterminals:
                #TODO: switch back to genexp
                prods = [p for p in self._prods_by_lhs(nonterm) 
                         if len(p.rhs()) == 2]

                # Handle productions of form A -> B C. Increment the count of
                # strings of length L derivable from A by the number of ways 
                # that B and C can combine to form a string of length L.
                for prod in prods:
                    B = prod.rhs()[0]
                    C = prod.rhs()[1]
                    for k in range(1, L):
                        left = self._counts[B][k]
                        right = self._counts[C][L - k]
                        self._counts[nonterm][L] = self._counts[nonterm][L] +  \
                                                   left * right

        # Check that the number of counts for a nonterminal is equal to
        # `new_length` (with 1 added for the None value). Then reset self.length
        # to reflect that.
        assert len(self._counts.itervalues().next()) == new_length + 1
        self.length = new_length

    def _preprocess(self, length):
        """Populate self._counts.

        _counts is a dict from nltk.grammar.Nonterminal to lists. Each index
        in the list holds the number of strings of length index that can be
        generated starting from the Nonterminal.
        """
        # Set the 0th value of self._counts to None since these values should
        # never be used. The lists need to be initialized so they can be used
        # below.
        for nonterm in self.nonterminals:
            self._counts[nonterm] = [None, 0]
        self.length = 1

        # For each production A -> 'a', increment the number of strings of
        # length one that can be generated from A.
        for prod in [p for p in self.productions if len(p.rhs()) == 1]:
            self._counts[prod.lhs()][1] += 1

        # Recursively find and set counts for lengths up to `length`
        self._update_counts(length)

class GenerationFailure(Exception):
    """Raised when a grammar can't generate a string of the requested length."""
    def __init__(self, length):
        self.length = length

    def __str__(self):
        return "GenerationFailure: length %d" % self.length



def _choose(pairs):
    """Choose at random from items and their probabilities of being chosen.

    pairs = [(item, probability), ...]
    """
    assert pairs
    the_sum = sum([pair[1] for pair in pairs])
    if the_sum == 0:
        return None
    if not .99 < the_sum < 1.01:
        raise ValueError('Probabilities must add to 1.')
        # NOTE: I should put in a range for this since sum won't be exact.
    r = random.uniform(0, the_sum)
    current = 0
    for (item, prob) in pairs:
        current += prob
        if r <= current:
            return item
     


def test_convert():
    g = nltk.data.load('file:noncnf.cfg')
    print '*' * 12 + '\n' +  str(g) + '\n' + '*' * 12
    cnf = convert_to_cnf(g)


class RegexToCFG(object):
    """Conversion of basic REs to their corresponding CFGs.
    """
    
class StubError(Exception):
    pass


def test_gen():
    #generator = CFLGeneration('g.cfg', 3)
    generator = CFLGenerator('g.cfg')
    s = generator.generate(4)
    print s

def main(argv):
    """
    python cfl.py gram.cfg --number 100 --length 4
    python cfl.py gram.cfg --number 47 --lowerlength 4 --upperlength 10
    """
    # args are grammar files
    # option for length, defaults to random number between 1 and 10
    # option for number of strings to generate, default=1
    # option for output: to string, json, ...?
    parser = OptionParser()
    parser.add_option('-n', '--number', dest='number', action='store', 
                      default=1, metavar='<NUMBER>', type='int',
                      help='')
    parser.add_option('-l', '--length', dest='length', action='store', 
                       default=None, metavar='<STRING LENGTH>', type='int',
                       help='')
    parser.add_option('-f', '--format', action='store', default='string', 
                      metavar='<FORMAT>', dest='the_format', 
                      help="Output format. Must be 'json' or 'string'.")
    parser.add_option('-o', '--outfile', action='store', default='-',
                      metavar='<FILENAME>', 
                      help="Filename of output. Defaults to stdout ('-').")

    options, args = parser.parse_args(argv)
    if options.the_format not in ('string', 'json'):
        parser.error("Argument of option --fmrat must be 'string' or 'json'.")

    if options.outfile == '-':
        out = sys.stdout
    else:
        out = open(options.outfile, 'w')

    if len(args) != 1: 
        parser.error("Only one grammar can be used.")
    results = []
    if options.length:
        length = lambda: options.length
        generator = CFLGenerator(args[0], options.length)
    else:
        length = lambda: random.randint(1, 10)
        generator = CFLGenerator(args[0], 10)
    results = [generator.generate(length()) for i in range(options.number)]
    if options.the_format == 'string':
        for res in results:
            print res
    elif options.the_format == 'json':
        print simplejson.dumps(results)


if __name__ == '__main__':
    exit(main(sys.argv[1:]))
    test_gen()
